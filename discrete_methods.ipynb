{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic Methods (Discrete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "import env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Anomaly Detection:** Identification of items, events or observations which is significantly different from the remaining data.\n",
    "\n",
    "- Non-parametric approach\n",
    "- Frequency or counting based\n",
    "    - How many time a value of variable (e.g. ip address) shows up\n",
    "    - More frequent - less likely to be an anomaly\n",
    "    - less frequent - more likely to be an anomaly\n",
    "    - Calculate probability \n",
    "\n",
    "    \n",
    "- Conditional probability \n",
    "    $$ {P(A|B) = }\\frac{\\text{P(A U B)}}{\\text{P(B)}} $$\n",
    "    \n",
    "    \n",
    " Examples: \n",
    "- How many times we see an ip address in the dataset (count)\n",
    "- What is probability of ip address showing up in the dataset (ip count / total observations)\n",
    "- Conditional probability. Given an ip address, what is prob of a particular status(e.g authentication failure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrangle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data comes from the webserver logs of the API that we used in the timeseries module. Each row is one request to the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f'mysql+pymysql://{env.user}:{env.password}@{env.host}/logs'\n",
    "df = pd.read_sql('SELECT * FROM api_access', url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can isolate the single column as an array and inspect the first couple of elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['entry'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['entry'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each element is a string and it appears that whitespaces separate key parts of that string. Let's use the `split()` function to chop our string up into an array of those parts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['entry'][0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the output of the split function on a single element to a variable\n",
    "parts = df['entry'][0].split()\n",
    "\n",
    "# Create an dictionary to store labeled components of our log data\n",
    "output = {}\n",
    "output['ip'] = parts[0]\n",
    "output['timestamp'] = parts[3][1:].replace(':', ' ', 1) # We can specify the replace function to only replace the first occurance\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have identified a process for splitting a string up into labeled parts, lets create a function that we can apply to all of our observations, comprehensively labeling each piece of information available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to deal with parsing one entry in our log data\n",
    "def parse_log_entry(entry):\n",
    "    parts = entry.split()\n",
    "    output = {}\n",
    "    output['ip'] = parts[0]\n",
    "    output['timestamp'] = parts[3][1:].replace(':', ' ', 1)\n",
    "    output['request_method'] = parts[5][1:]\n",
    "    output['request_path'] = parts[6]\n",
    "    output['http_version'] = parts[7][:-1]\n",
    "    output['status_code'] = parts[8]\n",
    "    output['size'] = int(parts[9])\n",
    "    output['user_agent'] = ' '.join(parts[11:]).replace('\"', '')\n",
    "    return pd.Series(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.entry.apply(parse_log_entry) # Applying the function to the entire dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add some synthetic anomalies to find in a bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Synthetic data\n",
    "new = pd.DataFrame([\n",
    "    [\"95.31.18.119\", \"21/Apr/2019 10:02:41\", \"GET\", \"/api/v1/items/\", \"HTTP/1.1\", '200', 1153005, \"python-requests/2.21.0\"],\n",
    "    [\"95.31.16.121\", \"17/Apr/2019 19:36:41\", \"GET\", \"/api/v1/sales?page=79/\", \"HTTP/1.1\", '301', 1005, \"python-requests/2.21.0\"],\n",
    "    [\"97.105.15.120\", \"18/Apr/2019 19:42:41\", \"GET\", \"/api/v1/sales?page=79/\", \"HTTP/1.1\", '301', 2560, \"python-requests/2.21.0\"],\n",
    "    [\"97.105.19.58\", \"19/Apr/2019 19:42:41\", \"GET\", \"/api/v1/sales?page=79/\", \"HTTP/1.1\", '200', 2056327, \"python-requests/2.21.0\"],\n",
    "], columns=df.columns)\n",
    "\n",
    "df = df.append(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we have some non-unique index elements. We could fix this with a `sort_index()`, but we can also just use a datetime index and those can handle non-unique elements just fine. Let's do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['size_mb'] = df['size'] / 1024 / 1024 # adding a new column scaled to megabytes for ease of understanding\n",
    "df.timestamp = pd.to_datetime(df.timestamp)\n",
    "df = df.set_index('timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting Anomalies in Discrete Variables\n",
    "\n",
    "- **count**: the number of times each unique value appears in the dataset\n",
    "- **frequencies**: the number of times each unique value appears in the dataset as a percentage of the total; the count divided by the total number of observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ip.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ip_df = pd.DataFrame(df.ip.value_counts(dropna=False)).reset_index().\\\n",
    "                rename(columns={'index': 'ip', 'ip': 'count'})\n",
    "ip_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ip.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate probabity for each ip \n",
    "\n",
    "# ip_prob = count for each ip / total count in the dataframe\n",
    "\n",
    "ip_df2 = pd.DataFrame((df.ip.value_counts(dropna=False))/df.ip.count()).reset_index().\\\n",
    "                rename(columns={'index': 'ip', 'ip': 'proba'})\n",
    "ip_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "11999 / 13978"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the two data frames create above into a single one:\n",
    "ip_df = ip_df.merge(ip_df2, on='ip')\n",
    "ip_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we can find how many unique ip addresses there are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_df.set_index('ip')['count'].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_df.set_index('ip')['count'].sort_values().tail(5).plot.barh(figsize=(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_df.set_index('ip')['count'].sort_values().plot.barh(figsize=(5,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Takeaways**\n",
    "At this point, we have a list of ip addresses that can be considered anomalous purely based of how infrequently they appear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Probabilities - Intersectionality\n",
    "\n",
    "- What is probability of a certain status code given an IP address?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTTP Status Codes\n",
    "\n",
    "- 200: ok\n",
    "- 3xx: redirects\n",
    "- 4xx: client level errors -- the requester did something wrong\n",
    "- 5xx: server level errors -- the server did something wrong\n",
    "\n",
    "### Conditional Probability Formula:\n",
    "                \n",
    "                prob(A|B) = prob(A & B)/prob(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_a_and_b = df.groupby(['ip', 'status_code']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_a_and_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_a_and_b = df.groupby(['ip', 'status_code']).size()/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_a_and_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_b = df.groupby('ip').size()/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_a_and_b / p_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a particular ip, what is probability of a certain status code\n",
    "status_given_ip = (\n",
    "    df.groupby('ip')\\\n",
    "    .status_code.value_counts(normalize=True)\\\n",
    "    .rename('proba_status_given_ip')\\\n",
    "    .reset_index())\n",
    "status_given_ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_given_ip[status_given_ip.proba_status_given_ip < 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.ip == '72.181.113.170'].sort_values(by='status_code', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Cases where the probability is < 100%\n",
    "* Status codes other than 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_given_ip[status_given_ip.status_code != '200']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How do we use this if we are looking to apply assumptions on unseen data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.sort_index().head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_index().tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a training set (removing the fake data I added earlier)\n",
    "train = df.loc['2019-04-16 19:34:42':'2019-04-17 12:55:14']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train), len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join the probabilities of these events to the original dataframe to detect anomalous events:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "status_given_ip # Essentially a list of the probability of each ip/status code combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index().merge(status_given_ip, on=['ip', 'status_code'], how='left').fillna(value=0).set_index('timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.proba_status_given_ip < 0.15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All low probability combinations are now easily identifiable with all additional data intact.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
