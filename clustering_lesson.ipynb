{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect Anomalies Using Density Based Clustering\n",
    "\n",
    "\n",
    "## Clustering-Based Anomaly Detection\n",
    "\n",
    "- Assumption: Data points that are similar tend to belong to similar groups or clusters, as determined by their distance from local centroids. Normal data points occur around a dense neighborhood and abnormalities are far away.\n",
    "\n",
    "- Using density based clustering, like DBSCAN, we can design the model such that the data points that do not fall into a cluster are the anomalies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# DBSCAN import\n",
    "from sklearn.cluster import DBSCAN\n",
    "# Scaler import\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import .txt file and convert it to a DataFrame object\n",
    "df = pd.read_table(\"anonymized-curriculum-access.txt\", sep = '\\s', header = None, \n",
    "                   names = ['date', 'time', 'page', 'id', 'cohort', 'ip'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's examine the head of the dataframe to make sure its\n",
    "# what we were expecting\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What do we know about this data set?\n",
    " - We are examining curriculum access at codeup.com\n",
    " - What do we want to find out about this data set?\n",
    "     - high density page views on certain dates?\n",
    "     - users that have a high number of page views?\n",
    "     - cohorts that have unusual activity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I know that I have the following columns here:\n",
    "# id: the number associated with the person viewing the curriculum\n",
    "# page: the url endpoint associated with the page being visited\n",
    "# cohort: number associated with the cohort that the id is associated with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets do a little aggregation based on the student id's in the data set,\n",
    "# focusing on the number of unique hits\n",
    "id_counts = df.groupby(['id'])['date', 'page', 'cohort'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my index is the user id\n",
    "# and my aggregated columns represent the unique\n",
    "# number of days, pages, and cohorts associated with each\n",
    "id_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_counts.date.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaways:\n",
    "- id #1 is likely a curriculum developer or someone involved on the Codeup side.\n",
    "- We have some instances near the bottom of a single or < week number of page/day access\n",
    "- What different values can we associate with multiple cohort assignments?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's observe unique hits based on cohort\n",
    "cohort_counts = df.groupby('cohort')['id', 'date','page'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - I want to observe the initial visit per user in this data set.\n",
    "     - How am I going to do this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's go back to our original dataframe and \n",
    "# convert to a datetime\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert our date to a pandas datetime so we can take the minimum\n",
    "# value\n",
    "df['date'] = pd.to_datetime(df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_access = df.groupby('id')['date'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thoughts: \n",
    "- Can I use this series to examine when cohorts potentially start inside this data set? \n",
    "- Let's use this series to break out that index and then regroup based on the first access date for each user\n",
    "    - Utilize the existing index by turning it back into a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's utilize that index that exists as the id, pop it back out into \n",
    "# a more columnar status, and then proceed forward with observing \n",
    "# high volume dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_by_first_access_date = pd.DataFrame({'first_access_date': first_access}).reset_index()\n",
    "id_by_first_access_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_by_first_access_date = id_by_first_access_date.groupby('first_access_date').count()\\\n",
    ".rename(columns={'id':'count_of_unique_ids'})\n",
    "id_by_first_access_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(id_by_first_access_date)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Number of First Access Users by Date')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Takeaways:\n",
    "     - It appears that there is a pretty clear pattern of multiple users starting between strong periods of lag.  It seems that we could determine when cohorts start based on this information and corroborate with any outside information sources to examine if mass curriculum access happened outside of the expectected or anticipated window for said curriculum access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could someone be stealing the content of our curriculum for their benefit beyond personal education? If so, we would probably see them accessing a large number of unique pages. I would imagine they wouldn't spend much time on each page, maybe taking screen shots, copy/paste or downloading the content. Let's take a look. \n",
    "\n",
    "Aggregate and compute 2 features...number of unique pages and total page views. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make an examination:\n",
    "# we want to look at individual users,\n",
    "# and I want to know how they interact with pages in the curriculum,\n",
    "# the number of unique pages and the number of total pages\n",
    "page_views = df.groupby(['id'])['page'].agg(['count', 'nunique'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_views.sort_values(by='count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(211)\n",
    "page_views['count'].hist(bins=50)\n",
    "plt.title('Distribution of Total Page Views Per User')\n",
    "\n",
    "plt.subplot(212)\n",
    "page_views['nunique'].hist(bins=50)\n",
    "plt.title('Distribution of Unique Page Views Per User')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's narrow down the scope\n",
    "# if we want to examine the users that have a lower count\n",
    "# but a high nunique,\n",
    "# we observed with our value counts and histograms previously\n",
    "# that we had a range of approxiately 200 for each of those features\n",
    "# per user\n",
    "# lets narrow down to users that have less than 200 page view total counts\n",
    "# but have over 190\n",
    "page_views[(page_views['count'] < 600) & (page_views['nunique'] > 190)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a sense, we have clustered our data using a rule derived from visual observation of the distributions of `count` and `nunique`. Our clusters might be broadly defined as:\n",
    "1. Anomalous (View Count < 600 & Unique Pages Viewed > 190)\n",
    "2. Non-anomalous (Everything else)\n",
    "\n",
    "Based on our criteria, we identified two anomalous users.\n",
    "\n",
    "But what if we are limited in domain expertise and/or would prefer to use a more sophisticated algorithm to segment/cluster our data? \n",
    "\n",
    "We can use **DBSCAN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale each attribute linearly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the scaler\n",
    "scaler = MinMaxScaler().fit(page_views)\n",
    "# use the scaler\n",
    "scaled_page_views = scaler.transform(page_views)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_page_views[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whip up some new column names\n",
    "scaled_cols = [col + '_scaled' for col in page_views.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_page_views_df = pd.DataFrame(scaled_page_views, columns=scaled_cols, index=page_views.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_page_views_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`count_scaled` and `nunique_scaled` can be plotted against each other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 2))\n",
    "sns.scatterplot(data = scaled_page_views_df, x ='count_scaled', y = 'nunique_scaled')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a DBSCAN object that requires a minimum of 20 data points in a neighborhood of radius 0.1 to be considered a core point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the object first\n",
    "dbsc = DBSCAN(min_samples=20, eps=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbsc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the object like we normally would with sklearn\n",
    "dbsc.fit(scaled_page_views_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbsc.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the scaled and non-scaled values into one dataframe\n",
    "page_views_total = page_views.merge(\n",
    "    scaled_page_views_df, on=page_views.index).drop(\n",
    "    columns=['key_0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check for the shape of the df:\n",
    "page_views_total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_views_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's apply the dbscan labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_views_total['labels'] = dbsc.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_views_total.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_views_total.labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "page_views_total[page_views_total.labels == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_views = page_views_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's look at the descriptive stats for the entire population, the inliers, then the outliers/anomalies\n",
    "print(\"Full Dataset\")\n",
    "print(page_views.describe())\n",
    "print(\"-------------\")\n",
    "print(\"Inliers Only\")\n",
    "print(page_views[page_views.labels==0].describe())\n",
    "print(\"-------------\")\n",
    "print(\"Outliers Only\")\n",
    "print(page_views[page_views.labels==-1].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(page_views['count'],\n",
    "           page_views['nunique'],\n",
    "           c=page_views['labels'])\n",
    "plt.xlabel('Count of pages viewed')\n",
    "plt.ylabel('Number of unique instances')\n",
    "plt.title('Cluster assignment by page count and unique count per user')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Follow up questions:\n",
    "    - These unusual users dont seem to all be weird in the same way.\n",
    "    - What's different about these users specifically?\n",
    "    - Examine each user based on these parameters,\n",
    "    - See which url endpoints they are visiting\n",
    "    - Examine the cohorts those users are associated with\n",
    "    - Examine the dates associated with first access for those users\n",
    "    - Determine if these users may be employees, instructors, students that belonged to one or more cohorts, students that went through both programs, etc\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with the DBSCAN properties\n",
    "- Read up on the epsilon and min_samples arguments into DBSCAN at https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html\n",
    "- Experiment with altering the epsilon values (the `eps` argument holding the threshhold parameter). Run the models and visualize the results. What has changed? Why do you think that is?\n",
    "- Double the `min_samples` parameter. Run your model and visualize the results. Consider what changed and why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "**file name:** clustering_anomaly_detection.py or clustering_anomaly_detection.ipynb\n",
    "\n",
    "\n",
    "### Clustering - DBSCAN\n",
    "\n",
    "Ideas: \n",
    "\n",
    "Use DBSCAN to detect anomalies in curriculum access. \n",
    "\n",
    "Use DBSCAN to detect anomalies in other products from the customers dataset. \n",
    "\n",
    "Use DBSCAN to detect anomalies in number of bedrooms and finished square feet of property for the filtered dataset you used in the clustering project (single unit properties with a logerror).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
